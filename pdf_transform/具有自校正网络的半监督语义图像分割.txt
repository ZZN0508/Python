Semi-Supervised Semantic Image Segmentation with Self-correcting Networks 
具有自我校正网络的半监督语义图像分割
Mostafa S. Ibrahim∗ Simon Fraser University 
Mostafa S. Ibrahim ∗ Simon Fraser University
Arash Vahdat NVIDIA 
阿拉什·瓦赫达特（NVIDIA）
Mani Ranjbar Sportlogiq 
Mani Ranjbar Sportlogiq
William G. Macready Sanctuary AI 
威廉·G·麦克雷迪保护区AI
msibrahi@sfu.ca 
MSI bra Hi@舒服.擦\"]\n]\n]\n]\n,\"zh\",1,\"auto
avahdat@nvidia.com 
Ava很多at@NVIDIA.com\"]\n]\n]\n]\n,\"zh\",1,\"auto
mani@sportlogiq.com 
玛尼@sport log IQ.com\"]\n]\n]\n]\n,\"zh\",1,\"auto
wgm@sanctuary.ai 
王光美@sanctuary.爱\"]\n]\n]\n]\n,\"zh\",1,\"auto
Abstract 
抽象
Building a large image dataset with high-quality object masks for semantic segmentation is costly and time consuming. In this paper, we introduce a principled semisupervised framework that only uses a small set of fully supervised images (having semantic segmentation labels and box labels) and a set of images with only object bounding box labels (we call it the weak set). Our framework trains the primary segmentation model with the aid of an ancillary model that generates initial segmentation labels for the weak set and a self-correction module that improves the generated labels during training using the increasingly accurate primary model. We introduce two variants of the self-correction module using either linear or convolutional functions. Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our models trained with a small fully supervised set perform similar to, or better than, models trained with a large fully supervised set while requiring ∼7x less annotation effort. 
使用高质量的对象蒙版来构建大型图像数据集进行语义分割既昂贵又耗时。在本文中，我们介绍了一种原则上的半监督框架，该框架仅使用一小组完全受监督的图像（具有语义分割标签和框标签）和一组仅具有对象边界框标签的图像（我们称之为弱集）。我们的框架借助一个辅助模型（用于为弱集生成初始分割标签）和一个自校正模块（用于在训练期间使用越来越精确的主模型来改进生成的标签）来训练主要分割模型。我们使用线性或卷积函数介绍自校正模块的两个变体。在PASCAL VOC 2012和Cityscape数据集上进行的实验表明，我们训练的带\\u200b\\u200b有小型全监督集合的模型的性能类似于或优于训练有大型的完整监督集合的模型，而所需的标注工作却减少了约7倍。\"]\n]\n]\n]\n,\"zh\",1,\"auto
1. Introduction 
1.简介
Deep convolutional neural networks (CNNs) have been successful in many computer vision tasks including image classiﬁcation [28, 19, 76], object detection [45, 34, 43], semantic segmentation [4, 71, 9], action recognition [14, 25, 49, 55], and facial landmark localization [53, 69, 75]. However, the common prerequisite for all these successes is the availability of large training corpora of labeled images. Of these tasks, semantic image segmentation is one of the most costly tasks in terms of data annotation. For example, drawing a segmentation annotation on an object is on average ∼8x slower than drawing a bounding box and ∼78x slower than labeling the presence of objects in images [5]. As a result, most image segmentation datasets are orders of magnitude smaller than image-classiﬁcation datasets. In this paper, we mitigate the data demands of semantic segmentation with a semi-supervised method that leverages cheap object bounding box labels in training. This approach 
深卷积神经网络（CNN）已在许多计算机视觉任务中取得成功，包括图像分类[28，19，76]，对象检测[45，34，43]，语义分割[4，71，9]，动作识别[14]
∗Work done while interning at D-Wave Systems 
∗在D-Wave Systems实习时完成的工作
reduces the data annotation requirements at the cost of requiring inference of the mask label for an object within a bounding box. 
减少了对数据注释的要求，但需要为边界框内的对象推断出遮罩标签。
Current state-of-the-art semi-supervised methods typically rely on hand-crafted heuristics to infer an object mask inside a bounding box [41, 12, 26]. In contrast, we propose a principled framework that trains semantic segmentation models in a semi-supervised setting using a small set of fully supervised images (with semantic object masks and bounding boxes) and a weak set of images (with only bounding box annotations). The fully supervised set is ﬁrst used to train an ancillary segmentation model that predicts object masks on the weak set. Using this augmented data a primary segmentation model is trained. This primary segmentation model is probabilistic to accommodate the uncertainty of the mask labels generated by the ancillary model. Training is formulated so that the labels supplied to the primary model are reﬁned during training from the initial ancillary mask labels to more accurate labels obtained from the primary model itself as it improves. Hence, we call our framework a self-correcting segmentation model as it improves the weakly supervised labels based on its current probabilistic model of object masks. 
当前最新的半监督方法通常依靠手工制作的启发式方法来推断边界框内的对象蒙版[41、12、26]。相比之下，我们提出了一种原则框架，该框架使用一小组完全受监督的图像（带有语义对象蒙版和边界框）和一组弱图像（仅具有边界框注释）在半监督的环境中训练语义分割模型。完全监督集首先用于训练辅助分割模型，该模型可预测弱集上的对象蒙版。使用该扩充的数据，可以训练主要的分割模型。该主要分割模型具有适应由辅助模型产生的掩模标签的不确定性的概率。制定了培训计划，以便在培训期间将提供给主要模型的标签从最初的辅助蒙版标签细化为从主要模型本身获得的更准确的标签（随着改进）。因此，我们将我们的框架称为自校正分割模型，因为它基于其当前的对象蒙版概率模型改善了弱监督标签。\"]\n]\n]\n]\n,\"zh\",1,\"auto
We propose two approaches to the self-correction mechanism. Firstly, inspired by Vahdat [56], we use a function that linearly combines the ancillary and model predictions. We show that this simple and effective approach is the natural result of minimizing a weighted KullbackLeibler (KL) divergence from a distribution over segmentation labels to both the ancillary and primary models. However, this approach requires deﬁning a weight whose optimal value should change during training. With this motivation, we develop a second adaptive self-correction mechanism. We use CNNs to learn how to combine the ancillary and primary models to predict a segmentation on a weak set of images. This approach eliminates the need for a weighting schedule. 
我们提出了两种自我校正机制的方法。
Experiments on the PASCAL VOC and Cityscapes datasets show that our models trained with a small portion of fully supervised set achieve a performance comparable to (and in some cases better than) the models trained with 
在PASCAL VOC和Cityscapes数据集上进行的实验表明，使用一小部分完全受监督的集训练的模型所取得的性能可与（或在某些情况下要优于）通过训练的模型进行比较
112715 
112715\"]\n]\n]\n]\n,\"zh\",1,\"auto
all the fully supervised images. 
所有完全受监督的图像。
2. Related Work 
2.相关工作
Semantic Segmentation: Fully convolutional networks (FCNs) [37] have become indispensable models for semantic image segmentation. Many successful applications of FCNs rely on atrous convolutions [65] (to increase the receptive ﬁeld of the network without down-scaling the image) and dense conditional random ﬁelds (CRFs) [27] (either as post-processing [6] or as an integral part of the segmentation model [73, 33, 48, 36]). Recent efforts have focused on encoder-decoder based models that extract longrange information using encoder networks whose output is passed to decoder networks that generate a high-resolution segmentation prediction. SegNet [4], U-Net [46] and ReﬁneNet [32] are examples of such models that use different mechanisms for passing information from the encoder to the decoder.1 Another approach for capturing long-range contextual information is spatial pyramid pooling [29]. ParseNet [35] adds global context features to the spatial features, DeepLabv2 [7] uses atrous spatial pyramid pooling (ASPP), and PSPNet [71] introduces spatial pyramid pooling on several scales for the segmentation problem. While other segmentation models may be used, we employ DeepLabv3+ [9] as our segmentation model because it outperforms previous CRF-based DeepLab models using simple factorial output. DeepLabv3+ replaces Deeplabv3’s [8] backbone with the Xception network [10] and stacks it with a simple two-level decoder that uses lower-resolution feature maps of the encoder. 
语义分割：全卷积网络（FCN）[37]已成为语义图像分割必不可少的模型。 FCN的许多成功应用都依赖于无规则卷积[65]（在不缩小图像比例的情况下增加了网络的接收域）和密集的条件随机域（CRF）[27]（作为后处理[6]或作为细分模型[73、33、48、36]的组成部分）。最近的工作集中在基于编码器-解码器的模型上，该模型使用编码器网络提取远程信息，其输出被传递到生成高分辨率分割预测的解码器网络。 SegNet [4]，U-Net [46]和RefineNet [32]是使用不同机制将信息从编码器传递到解码器的此类模型的示例。1另一种捕获远程上下文信息的方法是空间金字塔池[ 29]。 ParseNet [35]在空间特征中添加了全局上下文特征，DeepLabv2 [7]使用了粗糙的空间金字塔池（ASPP），而PSPNet [71]在几个尺度上引入了空间金字塔池来解决分割问题。尽管可以使用其他细分模型，但我们使用DeepLabv3 + [9]作为我们的细分模型，因为它使用简单的因子输出优于以前的基于CRF的DeepLab模型。 DeepLabv3 +用Xception网络[10]取代了Deeplabv3的[8]骨干网，并将其与一个简单的两级解码器堆叠在一起，该解码器使用了较低分辨率的编码器特征图。\"]\n]\n]\n]\n,\"zh\",1,\"auto
Robust Training: Training a segmentation model from bounding box information can be formulated as a problem of robust learning from noisy labeled instances. Previous work on robust learning has focused on classiﬁcation problems with a small number of output variables. In this setting, a common simplifying assumption models the noise on output labels as independent of the input [40, 39, 42, 52, 70]. However, recent work has lifted this constraint to model noise based on each instance’s content (i.e., input-dependent noise). Xiao et al. [63] use a simple binary indicator function to represent whether each instance does or does not have a noisy label. Misra et al. [38] represent label noise for each class independently. Vahdat [56] proposes CRFs to represent the joint distribution of noisy and clean labels extending structural models [57, 58] to deep networks. Ren et al. [44] gain robustness against noisy labels by reweighting each instance during training whereas Dehghani et al. [13] reweight gradients based on a conﬁdence score on labels. Among methods proposed for label 
鲁棒训练：从边界框信息中训练分割模型可以公式化为从嘈杂的带有标签的实例中进行鲁棒学习的问题。以前关于稳健学习的工作集中在具有少量输出变量的分类问题上。在这种设置下，一个通用的简化假设将输出标签上的噪声建模为独立于输入的噪声[40、39、42、52、70]。但是，最近的工作取消了此约束，可以根据每个实例的内容对噪声进行建模（即，与输入有关的噪声）。肖等。 [63]使用一个简单的二进制指示符函数来表示每个实例是否带有噪声标签。 Misra等。 [38]分别代表每个类别的标签噪声。 Vahdat [56]提出了CRF来表示嘈杂和干净标签的联合分布，从而将结构模型[57，58]扩展到深层网络。任等人。文献[44]通过在训练过程中对每个实例进行加权来获得针对噪声标签的鲁棒性。 [13]根据标签上的置信度分数重新加权梯度。在建议的标签方法中\"]\n]\n]\n]\n,\"zh\",1,\"auto
1 SegNet [4] transfers max-pooling indices from encoder to decoder, U-Net [46] introduces skip-connections between encoder-decoder networks and ReﬁneNet [32] proposes multipath reﬁnement in the decoder through long-range residual blocks. 
1 SegNet [4]将最大池索引从编码器传输到解码器，U-Net [46]引入了编码器-解码器网络之间的跳过连接，RefinNet [32]提出了通过远程残差块在解码器中进行多路径优化。
correction, Veit et al. [59] use a neural regression model to predict clean labels given noisy labels and image features, Jiang et al. [24] learn curriculum, and Tanaka et al. [54] use the current model to predict labels. All these models have been restricted to image-classiﬁcation problems and have not yet been applied to image segmentation. 
校正，Veit等。
Semi-Supervised Semantic Segmentation: The focus of this paper is to train deep segmentation CNNs using bounding box annotations. Papandreou et al. [41] propose an Expectation-Maximization-based (EM) algorithm on top of DeepLabv1 [6] to estimate segmentation labels for the weak set of images (only with box information). In each training step, segmentation labels are estimated based on the network output in an EM fashion. Dai et al. [12] propose an iterative training approach that alternates between generating region proposals (from a pool of ﬁxed proposals) and ﬁne-tuning the network. Similarly, Khoreva et al. [26] use an iterative algorithm but rely on GrabCut [47] and handcrafted rules to extract the segmentation mask in each iteration. Our work differs from these previous methods in two signiﬁcant aspects: i) We replace hand-crafted rules with an ancillary CNN for extracting probabilistic segmentation labels for an object within a box for the weak set. ii) We use a self-correcting model to correct for the mismatch between the output of the ancillary CNN and the primary segmentation model during training. In addition to box annotations, segmentation models may use other forms of weak annotations such as image pixel-level [60, 62, 22, 3, 17, 61, 15], image labellevel [68], scribbles [64, 31], point annotation [5], or web videos [20]. Recently, adversarial learning-based methods [23, 51] have been also proposed for this problem. Our framework is complimentary to other forms of supervision or adversarial training and can be used alongside them. 
半监督语义分割：本文的重点是使用边界框注释来训练深度分割CNN。 Papandreou等。 [41]在DeepLabv1 [6]的基础上提出了一种基于期望最大化的算法（EM），以估计弱图像集的分割标签（仅包含框信息）。在每个训练步骤中，基于EM方式的网络输出估计分段标签。戴等。 [12]提出了一种迭代训练方法，该方法在生成区域建议（从一组固定建议中）和对网络进行微调之间交替。同样，Khoreva等。 [26]使用迭代算法，但依靠GrabCut [47]和手工制作的规则在每次迭代中提取分割蒙版。我们的工作与以前的方法在两个重要方面有所不同：i）我们用辅助CNN替换了手工规则，以提取弱集盒内对象的概率分割标签。 ii）我们使用自校正模型来校正训练过程中辅助CNN输出与主要分割模型之间的不匹配。除了框注解之外，分割模型还可以使用其他形式的弱注解，例如图像像素级[60、62、22、3、17、61、15]，图像标签级[68]，涂鸦[64、31]，点注释[5]或网络视频[20]。最近，针对该问题的基于对抗学习的方法[23，51]也被提出。我们的框架是对其他形式的监督或对抗性培训的补充，并且可以与它们一起使用。\"]\n]\n]\n]\n,\"zh\",1,\"auto
3. Methods 
3.方法
Our goal is to train a semantic segmentation network in a semi-supervised setting using two training sets: i) a small fully supervised set (containing images, segmentation ground-truth and object bounding boxes) and ii) a weak set (containing images and object bounding boxes only). An overview of our framework is shown in Fig. 1. There are three models: i) The Primary segmentation model generates a semantic segmentation of objects given an image. ii) The Ancillary segmentation model outputs a segmentation given an image and bounding box. The model generates an initial segmentation for the weak set, which aids training of the primary model. iii) The Self-correction module reﬁnes the segmentations generated by the ancillary and current primary model for the weak set. Both the ancillary and the primary models are based on DeepLabv3+ [9]. However, our framework is general and can use any existing segmentation model. 
我们的目标是使用两个训练集在半监督的情况下训练语义分割网络：i）一个小的完全监督的集合（包含图像，分割地面实况和对象边界框），ii）弱的集合（包含图像和
12716 
12716\"]\n]\n]\n]\n,\"zh\",1,\"auto
)
)\"]\n]\n]\n]\n,\"zh\",1,\"auto
F
F
(
(\"]\n]\n]\n]\n,\"zh\",1,\"auto
t
Ť
e
是
S
小号
-
-\"]\n]\n]\n]\n,\"zh\",1,\"auto
ll
二
uF
F
)
)\"]\n]\n]\n]\n,\"zh\",1,\"auto
W
在
(
(\"]\n]\n]\n]\n,\"zh\",1,\"auto
t
Ť
e
是
S
小号
Primary Segmentation	 Model 
主要细分模型
Pixelwise Prediction 
像素预测
h
H
c
C
t
Ť
a
一种
B
乙
t
Ť
n
ñ
e
是
rr
rr
u
ü
C
C
Ground Truth	 Labels 
地面真相标签
Cross	Entropy	 Loss 
交叉熵损失
Refined Soft Labels 
精致的软标签
t
Ť
npeodsar
恩佩德萨尔
t
Ť
i
一世
-
-\"]\n]\n]\n]\n,\"zh\",1,\"auto
kae
更改
W
在
Image 
图片
Bounding	Boxes 
边界框
Ancillary Segmentation Model 
辅助分割模型
Self	Correction Module 
自我校正模块
g
G
Figure 1: An overview of our segmentation framework consisting of three models: i) Primary segmentation model generates a semantic segmentation of objects given an image. This is the main model that is subject to the training and is used at test time. ii) Ancillary segmentation model outputs a segmentation given an image and bounding box. This model generates an initial segmentation for the weak set, which will aid training the primary model. iii) Self-correction module reﬁnes segmentations generated by the ancillary model and the current primary model for the weak set. The primary model is trained using the cross-entropy loss that matches its output to either ground-truth segmentation labels for the fully supervised examples or soft reﬁned labels generated by the self-correction module for the weak set. 
图1：我们的分割框架的概述，该分割框架由三个模型组成：i）主分割模型在给定图像的情况下生成对象的语义分割。
In Sec. 3.1, we present the ancillary model, and in Sec. 3.2, we show a simple way to use this model to train the primary model. In Sec. 3.3 and Sec. 3.4, we present two variants of self-correcting model. Notation: xxx represents an image, bbb represents object bounding boxes in an image, and yyy = [yyy1 , yyy2 , . . . , yyyM ] represents a segmentation label where yyym ∈ [0, 1]C+1 for m ∈ {1, 2, . . . , M } is a one-hot label for the mth pixel, C is the number of foreground labels augmented with the background class, and M is the total number of pixels. Each bounding box is associated with an object and has one of the foreground labels. The fully supervised dataset is indicated as F = {(xxx(f ) , yyy (f ) , bbb(f ) )}F f =1 where F is the total number of instances in F . Similarly, the weak set is noted by W = {(xxx(w) , bbb(w) )}W w=1 . We use p(yyy |xxx; φφφ) to represent the primary segmentation model and panc (yyy |xxx, bbb; θθθ) to represent the ancillary model. φφφ and θθθ are the respective parameters of each model. We occasionally drop the denotation of parameters for readability. We assume that both ancillary and primary models deﬁne a distribution of segmentation labels using a factorial distribution, 
在秒3.1，我们介绍辅助模型，并在第二节。 3.2，我们展示了一种使用此模型训练基本模型的简单方法。在秒3.3和秒3.4，我们提出了两种自校正模型。表示法：xxx表示图像，bbb表示图像中的对象边界框，并且yyy \\u003d [yyy1，yyy2，。 。 。 ，yyyM]表示一个分段标签，其中yyym∈[0，1] C + 1对于m∈{1,2，...。 。 。 ，M}是第m个像素的单标签，C是用背景类增强的前景标签的数量，M是像素的总数。每个边界框都与一个对象关联，并具有前景标签之一。完全监督的数据集表示为F \\u003d {（xxx（f），yyy（f），bbb（f））} F f \\u003d 1，其中F是F中实例的总数。类似地，弱集记为W \\u003d {（xxx（w），bbb（w））} W w \\u003d 1。我们使用p（yyy | xxx;φφφ）表示主要分割模型，并使用panc（yyy | xxx，bbb;θθθ）表示辅助模型。 φφφ和θθθ是每个模型的各自参数。为了便于阅读，我们有时会删除参数的符号。我们假设辅助模型和主要模型都使用阶乘分布定义了细分标签的分布，\"]\n]\n]\n]\n,\"zh\",1,\"auto
QM 
质量管理
i.e., p(yyy |xxx; φφφ) = QM m=1 pm (yyym |xxx; φφφ) and panc (yyy |xxx, bbb; θθθ) = m=1 panc,m (yyym |xxx, bbb; θθθ) where each factor (pm (yyym |xxx; φφφ) 
即p（yyy | xxx;φφφ）\\u003d QM m \\u003d 1 pm（yyym | xxx;φφφ）和panc（yyy | xxx，bbb;θθθ）\\u003d m \\u003d 1 panc，m（yyym | xxx，bbb;θθθ）
or panc,m (yyym |xxx, bbb; θθθ)) is a categorical distribution (over C + 1 categories). 
或panc，m（yyym | xxx，bbb;θθθ））是分类分布（在C +1个类别上）。
3.1. Ancillary Segmentation Model 
3.1。
The key challenge in semi-supervised training of segmentation models with bounding box annotations is to infer the segmentation of the object inside a box. Existing approaches to this problem mainly rely on hand-crafted rulebased procedures such as GrabCut [47] or an iterative label 
带有边界框注释的分段模型的半监督训练中的关键挑战是推断框内对象的分段。
reﬁnement [41, 12, 26] mechanism. This latter procedure typically iterates between segmentation extraction from the image and label reﬁnement using the bounding box information (for example, by zeroing-out the mask outside of boxes). The main issues with such procedures are i) bounding box information is not directly used to extract the segmentation mask, ii) the procedure may be suboptimal as it is hand-designed, and iii) the segmentation becomes ambiguous when multiple boxes overlap. In this paper, we take a different approach by designing an ancillary segmentation model that forms a per-pixel label distribution given an image and bounding box annotation. This model is easily trained using the fully supervised set (F ) and can be used as a training signal for images in W . At inference time, both the image and its bounding box are fed to the network to obtain panc (yyy |xxx(w) , bbb(w) ), the segmentation labels distribution. Our key observation in designing the ancillary model is that encoder-decoder-based segmentation networks typically rely on encoders initialized from an imageclassiﬁcation model (e.g., ImageNet pretrained models). This usually improves the segmentation performance by transferring knowledge from large image-classiﬁcation datasets. To maintain the same advantage, we augment an encoder-decoder-based segmentation model with a parallel bounding box encoder network that embeds bounding box information at different scales (See Fig. 2). The input to the bounding box encoder is a 3D tensor representing a binarized mask of the bounding boxes and a 3D shape representing the target dimensions for the encoder output. The input mask tensor is resized to the target shape then passed through a 3×3 convolution layer with sigmoid 
完善[41，12，26]机制。后面的过程通常在使用边界框信息（例如，通过将框外的遮罩清零）从图像的分割提取和标签修饰之间进行迭代。此类程序的主要问题是：i）边界框信息未直接用于提取分割蒙版； ii）该程序由于是手工设计的，因此可能不是最优的； iii）当多个框重叠时，分割变得模棱两可。在本文中，我们通过设计辅助分割模型来采用不同的方法，该模型在给定图像和边界框注释的情况下形成了每个像素的标签分布。使用完全监督集（F）可以轻松训练该模型，并且可以将其用作W中图像的训练信号。在推断时，图像及其边界框都被馈送到网络以获得panc（yyy | xxx（w），bbb（w）），即分割标签分布。我们在设计辅助模型时的主要观察结果是，基于编码器/解码器的分割网络通常依赖于从图像分类模型（例如ImageNet预训练模型）初始化的编码器。通常，这可以通过传输大型图像分类数据集中的知识来提高分割性能。为了保持相同的优势，我们使用并行的边界框编码器网络扩展了基于编码器-解码器的分割模型，该网络以不同的比例嵌入边界框信息（参见图2）。边界框编码器的输入是一个3D张量（表示边界框的二值化蒙版）和3D形状（表示编码器输出的目标尺寸）。将输入遮罩张量调整为目标形状，然后通过具有S形的3×3卷积层\"]\n]\n]\n]\n,\"zh\",1,\"auto
12717 
12717\"]\n]\n]\n]\n,\"zh\",1,\"auto
Segmentation Encoder 
分段编码器
S
小号
ca
那
l
升
e
是
2
2\"]\n]\n]\n]\n,\"zh\",1,\"auto
S
小号
ca
那
l
升
e
是
1
1\"]\n]\n]\n]\n,\"zh\",1,\"auto
box information guides the ancillary model to look for the object inside the box at inference time. The simplest approach to training the primary model is to train it to predict using ground-truth labels on the fully supervised set F and the labels generated by the ancillary model on the weak set W . For this “no-self-correction” model the Self-correction module in Fig. 1 merely copies the predictions made by the ancillary segmentation model. Training is guided by optimizing: 
框信息可指导辅助模型在推理时在框内查找对象。
Bounding Box Encoder  
边界盒编码器
Prediction 
预测
Segmentation Decoder 
分段解码器
Figure 2: An overview of the ancillary segmentation model. We modify an existing encoder-decoder segmentation model by introducing a bounding box encoder that embeds the box information. The output of the bounding box encoder after passing through a sigmoid activation acts as an attention map. Feature maps at different scales from the encoder are fused (using element-wise-multiplication) with attention maps, then passed to the decoder. 
图2：辅助细分模型的概述。
activations. The resulting tensor can be interpreted as an attention map which is element-wise multiplied to the feature maps generated by the segmentation encoder. Fig. 2 shows two paths of such feature maps at two different scales, as in the DeepLabv3+ architecture. For each scale, an attention map is generated, fused with the corresponding feature map using element-wise multiplication, and fed to the decoder. For an image of size W × H × 3, we represent its object bounding boxes using a binary mask of size W ×H×(C +1) that encodes the C + 1 binary masks. The cth binary mask at a pixel has the value 1 if it is inside one of the bounding boxes of the cth class. A pixel in the background mask has value 1 if it is not covered by any bounding box. The ancillary model is trained using the cross-entropy loss on the full dataset F : 
激活。
max 
最高
θθθ X 
θθθX
f ∈F 
f∈F
log panc (yyy (f ) |xxx(f ) , bbb(f ) ; θθθ), 
log panc（yyy（f）| xxx（f），bbb（f）;θθθ），
(1) 
(1)\"]\n]\n]\n]\n,\"zh\",1,\"auto
which can be expressed analytically under the factorial distribution assumption. This model is held ﬁxed for the subsequent experiments. 
可以在阶乘分布假设下以解析方式表示。
3.2. No Self(cid:173)Correction 
3.2。
We empirically observe that the performance of our ancillary model is superior to segmentation models that do not have box information. This is mainly because the bounding 
我们凭经验观察到，我们的辅助模型的性能优于没有框信息的细分模型。
max 
最高
φφφ X 
X
f ∈F 
f∈F
log p(yyy (f ) |xxx(f ) ; φφφ) + 
log p（yyy（f）| xxx（f）;φφφ）+
(2) 
(2)\"]\n]\n]\n]\n,\"zh\",1,\"auto
X
X
w∈W 
w∈W\"]\n]\n]\n]\n,\"zh\",1,\"auto
X
X
yyy 
y
panc (yyy |xxx(w) , bbb(w) ; θθθ) log p(yyy |xxx(w) ; φφφ), 
panc（yyy | xxx（w），bbb（w）;θθθ）log p（yyy | xxx（w）;φφφ），
where the ﬁrst term is the cross-entropy loss with one-hot ground-truth labels as target and the second term is the cross-entropy with soft probabilistic labels generated by panc as target. Note that the ancillary model parameterized by θθθ is ﬁxed. We call this approach the no self-correction model as it relies directly on the ancillary model for training the primary model for examples in W . 
其中第一个项是以一个热的地面真相标签为目标的交叉熵损失，第二个项是以panc为目标的软概率标签的交叉熵。
3.3. Linear Self(cid:173)Correction 
3.3。
Eq. 2 relies on the ancillary model to predict label distribution on the weak set. However, this model is trained using only instances of F without beneﬁt of the data in W . Several recent works [41, 12, 26, 54, 56] have incorporated the information in W by using the primary model itself (as it is being trained on both F and W ) to extract more accurate label distributions on W . Vahdat [56] introduced a regularized ExpectationMaximization algorithm that uses a linear combination of KL divergences to infer a distribution over missing labels for general classiﬁcation problems. The main insight is that the inferred distribution q(yyy |xxx, bbb) over labels should be close to both the distributions generated by the ancillary model panc (yyy |xxx, bbb) and the primary model p(yyy |xxx). However, since the primary model is not capable of predicting the segmentation mask accurately early in training, these two terms are reweighted using a positive scaling factor α: 
等式
min 
分
KL(q(yyy |xxx, bbb)||p(yyy |xxx))+αKL(q(yyy |xxx, bbb)||panc (yyy |xxx, bbb)). (3) 
KL（q（yyy | xxx，bbb）|| p（yyy | xxx））+αKL（q（yyy | xxx，bbb）|| panc（yyy | xxx，bbb））。
q
q
The global minimizer of Eq. 3 is obtained as the weighted geometric mean of the two distributions: 
等式的全局最小化器。
q(yyy |xxx, bbb) ∝ (cid:0)p(yyy |xxx)pα anc (yyy |xxx, bbb)(cid:1) 
q（yyy | xxx，bbb）∝（cid：0）p（yyy | xxx）pαanc（yyy | xxx，bbb）（cid：1）
1α+1 . 
1a + 1。
(4) 
(4)\"]\n]\n]\n]\n,\"zh\",1,\"auto
Since both panc (yyy |xxx, bbb) and p(yyy |xxx) decompose into a product of probabilities over the components of yyy , and since the distribution over each component is categorical, then m=1 qm (yyym |xxx, bbb) is also factorial where the parameters of the categorical distribution over each component are computed by applying softmax activation to the 
由于panc（yyy | xxx，bbb）和p（yyy | xxx）都分解为yyy分量的概率乘积，并且由于每个分量的分布都是分类的，因此m \\u003d 1 qm（yyym | xxx，bbb
q(yyy |xxx, bbb) = QM 
q（yyy | xxx，bbb）\\u003d QM
12718 
12718\"]\n]\n]\n]\n,\"zh\",1,\"auto
m (cid:1)/(cid:0)α + 1(cid:1)(cid:17). Here, σ(.) 
m（cid：1）/（cid：0）α+ 1（cid：1）（cid：17）。
linear combination of logits coming from primary and ancillary models using σ(cid:16)(cid:0)lllm + α lllanc is the softmax function and, lllm and lllanc m are logits generated by primary and ancillary models for the mth pixel. Having ﬁxed q(yyy |xxx(w) , bbb(w) ) on the weak set in each iteration of training the primary model, we can train the primary model using: 
使用σ（cid：16）（cid：0）lllm +αlllanc来自主模型和辅助模型的logit的线性组合是softmax函数，lllm和lllanc m是由第m个像素的主模型和辅助模型生成的logit。
max 
最高
φφφ X 
X
F
F
log p(yyy (f ) |xxx(f ) ; φφφ) + 
log p（yyy（f）| xxx（f）;φφφ）+
(5) 
(5)\"]\n]\n]\n]\n,\"zh\",1,\"auto
X
X
W
在
X
X
yyy 
y
q(yyy |xxx(w) , bbb(w) ) log p(yyy |xxx(w) ; φφφ). 
q（yyy | xxx（w），bbb（w））log p（yyy | xxx（w）;φφφ）。
Note that α in Eq. 3 controls the closeness of q to p(yyy |xxx) 
注意，等式中的α。
and panc (yyy |xxx, bbb). With α = ∞, we have q = panc (yyy |xxx, bbb) 
和panc（yyy | xxx，bbb）。
and the linear self-correction in Eq. 5 collapses to Eq. 2, whereas α = 0 recovers q = p(yyy |xxx). A ﬁnite α maintains q close to both p(yyy |xxx) and panc (yyy |xxx, bbb). At the beginning of training, panc (yyy |xxx, bbb) cannot predict the segmentation label distribution accurately. Therefore, we deﬁne a schedule for α where α is decreased from a large value to a small value during training of the primary model. This corrective model is called the linear self-correction model as it uses the solution to a linear combination of KL divergences (Eq. 3) to infer a distribution over latent segmentation labels.2 As the primary model’s parameters are optimized during training, α biases the self-correction mechanism towards the primary model. 
和方程式中的线性自校正
3.4. Convolutional Self(cid:173)Correction 
3.4。
One disadvantage of linear self-correction is the hyperparameter search required for tuning the α schedule during training. In this section, we present an approach that overcomes this difﬁculty by replacing the linear function with a convolutional network that learns the self-correction mechanism. As a result, the network automatically tunes the mechanism dynamically as the primary model is trained. If the primary model predicts labels accurately, this network can shift its predictions towards the primary model. Fig. 3 shows the architecture of the convolutional selfcorrecting model. This small network accepts the logits generated by panc (yyy |xxx, bbb) and p(yyy |xxx) models and generates the factorial distribution qconv (yyy |xxx, bbb; λλλ) over segmentation labels where λλλ represents the parameters of the subnetwork. The convolutional self-correction subnetwork consists of two convolution layers. Both layers use a 3×3 kernel and ReLU activations. The ﬁrst layer has 128 output feature maps and the second has feature maps based on the number of classes in the dataset. 
线性自校正的一个缺点是训练期间调整α计划所需的超参数搜索。在本节中，我们介绍一种通过用学习自校正机制的卷积网络代替线性函数来克服这一难题的方法。结果，在训练主要模型时，网络会自动动态调整该机制。如果主要模型可以准确地预测标签，则该网络可以将其预测移向主要模型。图3显示了卷积自校正模型的体系结构。这个小型网络接受由panc（yyy | xxx，bbb）和p（yyy | xxx）模型生成的对数，并在分段标签上生成阶乘分布qconv（yyy | xxx，bbb;λλλ），其中λλλ代表子网的参数。卷积自校正子网络由两个卷积层组成。这两层都使用3×3内核和ReLU激活。第一层具有128个输出特征图，第二层具有基于数据集中的类数的特征图。\"]\n]\n]\n]\n,\"zh\",1,\"auto
2 In principal, logits of qm (yyym |xxx, bbb) can be obtained by a 1×1 convolutional layer applied to the depth-wise concatenation of lll and lllanc with a ﬁxed averaging kernel. This originally motivated us to develop the convolutional self-correction model in Sec. 3.4 using trainable kernels. 
2原则上，qm的对数（yyym | xxx，bbb）可以通过将1×1卷积层应用于固定的平均内核，将其应用于lll和lllanc的深度级联。
The challenge here is to train this subnetwork such that it predicts the segmentation labels more accurately than either panc (yyy |xxx, bbb) or p(yyy |xxx). To this end, we introduce an additional term in the objective function, which trains the subnetwork using training examples in F while the primary model is being trained on the whole dataset: 
这里的挑战是训练该子网，使其比panc（yyy | xxx，bbb）或p（yyy | xxx）更准确地预测分段标签。
max 
最高
φφφ,λλλ X 
φφφ，λλλX
F
F
log p(yyy (f ) |xxx(f ) ; φφφ) + 
log p（yyy（f）| xxx（f）;φφφ）+
(6) 
(6)\"]\n]\n]\n]\n,\"zh\",1,\"auto
X
X
W
在
X
X
yyy 
y
qconv (yyy |xxx(w) , bbb(w) ; λλλ) log p(yyy |xxx(w) ; φφφ) + 
qconv（yyy | xxx（w），bbb（w）;λλλ）log p（yyy | xxx（w）;φφφ）+
log qconv (yyy (f ) |xxx(f ) , bbb(f ) ; λλλ), 
log qconv（yyy（f）| xxx（f），bbb（f）;λλλ），
X
X
F
F
where the ﬁrst and second terms train the primary model on F and W (we do not backpropagate through q in the second term) and the last term trains the convolutional selfcorrecting network. Because the qconv subnetwork is initialized randomly, it is not able to accurately predict segmentation labels on W early on during training. To overcome this issue, we propose the following pretraining procedure: 
其中第一和第二项训练关于F和W的主要模型（我们在第二项中不通过q向后传播），而最后一项训练卷积自校正网络。
1. Initial training of ancillary model: As with the previous self-correction models, we need to train the ancillary model. Here, half of the fully supervised set (F ) is used for this purpose. 
1.辅助模型的初始训练：与以前的自校正模型一样，我们需要训练辅助模型。
2. Initial training of conv. self-correction network: The fully supervised data (F ) is used to train the primary model and the convolutional self-correcting network. This is done using the ﬁrst and last terms in Eq. 6. 
2.转换的初步培训
3. The main training: The whole data (F and W ) are used to ﬁne-tune the previous model using the objective function in Eq. 6. 
3.主要训练：整个数据（F和W）用于使用公式中的目标函数微调以前的模型。
The rationale behind using half of F in stage 1 is that if we used all F for training the panc (yyy |xxx, bbb) model, it would train to predict the segmentation mask almost perfectly on this set, therefore, the subsequent training of the convolutional self-correcting network would just learn to rely on panc (yyy |xxx, bbb) . To overcome this training issue, the second half of F is held out to help the self-correcting network to 
在阶段1中使用一半F的基本原理是，如果我们使用所有F来训练panc（yyy | xxx，bbb）模型，它将训练以在该集合上几乎完美地预测分割蒙版，因此，随后的
learn how to combine panc (yyy |xxx, bbb) and p(yyy |xxx). 
了解如何组合panc（yyy | xxx，bbb）和p（yyy | xxx）。
4. Experiments 
4.实验
In this section, we evaluate our models on the PASCAL VOC 2012 and Cityscapes datasets. Both datasets contain object segmentation and bounding box annotations. We split the full dataset annotations into two parts to simulate a fully and semi-supervised setting. Similar to [9, 41], performance is measured using the mean intersection-overunion (mIOU) across the available classes. 
在本节中，我们在PASCAL VOC 2012和Cityscapes数据集上评估模型。
12719 
12719\"]\n]\n]\n]\n,\"zh\",1,\"auto
Primary Logits 
主要登录
Ancillary Logits 
辅助登录
3×3  Conv 
3×3转换
σ
σ
3×3  Conv 
3×3转换
Refined  Soft  Labels 
精致的软标签
Figure 3: Convolutional self-correction model learns reﬁning the input label distributions. The subnetwork receives logits from the primary and ancillary models, then concatenates and feeds the output to a two-layer CNN. 
图3：卷积自校正模型学习精炼输入标签的分布。
Training: We use the public Tensorﬂow [1] implementation of DeepLabv3+ [9] as the primary model. We use an initial learning rate of 0.007 and train the models for 30,000 steps from the ImageNet-pretrained Xception-65 model [9].3 For all other parameters we use standard settings suggested by other authors. At evaluation time, we apply ﬂipping and multi-scale processing for images as in [9]. We use 4 GPUs, each with a batch of 4 images. We deﬁne the following baselines in all our experiments: 
培训：我们使用DeepLabv3 + [9]的公共Tensor1ow [1]实现作为主要模型。
1. Ancillary Model: This is the ancillary model, introduced in Sec. 3.1, predicts semantic segmentation labels given an image and its object bounding boxes. This model is expected to perform better than other models as it uses bounding box information. 
1.辅助模型：这是在第二节中介绍的辅助模型。
2. No Self-correction: This is the primary model trained using the model introduced in Sec. 3.2. 
2.无自我校正：这是使用本节介绍的模型训练的主要模型。
3. Lin. Self-correction: This is the primary model trained with linear self-correction as in Sec. 3.3. 
3.林。
4. Conv. Self-correction: The primary model trained with the convolutional self-correction as in Sec. 3.4. 
4.转换
5. EM-ﬁxed Baseline: Since our linear self-correction model is derived from a regularized EM model [56], we compare our model with Papandreou et al. [41] which is also an EM-based model. We implemented their EM-ﬁxed baseline with DeepLabv3+ for fair comparison. This baseline achieved the best results in [41] for semi-supervised learning. 
5. EM固定基线：由于我们的线性自校正模型是从正规化的EM模型得出的[56]，因此我们将模型与Papandreou等进行了比较。
4.1. PASCAL VOC Dataset 
4.1。
In this section, we evaluate all models on the PASCAL VOC 2012 segmentation benchmark [16]. This dataset consists of 1464 training, 1449 validation, and 1456 test images covering 20 foreground object classes and one background class for segmentation. An auxiliary dataset of 9118 training images is provided by [18]. We suspect, however, that the segmentation labels of [18] contain a small amount of noise. In this section, we refer to the union of the original PASCAL VOC training dataset and the auxiliary set as the training set. We evaluate the models mainly on the validation set and the best model is evaluated only once on the test set using the online evaluation server. In Table 1, we show the performance of different variants of our model for different sizes of the fully supervised set F . The remaining examples in the training set are used as W . We make several observations from Table 1: i) The ancillary model that predicts segmentation labels given an image and its object bounding boxes performs well even when it is trained with a training set as small as 200 images. This shows that this model can also provide a good training signal for the weak set that lacks segmentation labels. ii) The linear self-correction model typically performs better than no self-correction model supporting our idea that combining the primary and ancillary model for inferring segmentation labels results in better training of the primary model. iii) The convolutional self-correction model performs comparably or better than the linear self-correction while eliminating the need for deﬁning an α schedule. Fig. 4 shows the output of these models. 
在本节中，我们根据PASCAL VOC 2012细分基准[16]评估所有模型。该数据集包括1464个训练，1449个验证和1456个测试图像，这些图像涵盖20个前景对象类和一个用于分割的背景类。 [18]提供了9118个训练图像的辅助数据集。但是，我们怀疑[18]的分段标签包含少量噪声。在本节中，我们将原始PASCAL VOC训练数据集与辅助集的并集称为训练集。我们主要在验证集中评估模型，而使用在线评估服务器在测试集中仅对最佳模型评估一次。在表1中，我们显示了模型的不同变体对于完全受监督的集合F的不同大小的性能。训练集中的其余示例用作W。我们从表1中得到一些观察结果：i）预测给定图像及其对象边界框的分割标签的辅助模型即使在使用小至200张图像的训练集进行训练时也能很好地发挥作用。这表明该模型还可以为缺少分段标签的弱集提供良好的训练信号。 ii）线性自校正模型通常比没有自校正模型表现更好，这支持了我们的思想，即结合主模型和辅助模型来推断分割标签可以更好地训练主模型。 iii）卷积自校正模型的性能与线性自校正相当或更好，同时无需定义α调度表。图4显示了这些模型的输出。\"]\n]\n]\n]\n,\"zh\",1,\"auto
# images in F 200 400 800 1464 Ancillary Model 81.57 83.56 85.36 86.71 No Self-correction 78.75 79.19 80.39 80.34 Lin. Self-correction 79.43 79.59 80.69 81.35 Conv. Self-correction 78.29 79.63 80.12 82.33 
F 200400800 1464辅助型号中的＃个图像81.57 83.56 85.36 86.71无自校正78.75 79.19 80.39 80.34 Lin。
Table 1: Ablation study of models on the PASCAL VOC 2012 validation set using mIOU for different sizes of F . For the last three rows, the remaining images in the training set is used as W , i.e. W + F = 10582. 
表1：对于不同大小的F，使用mIOU在PASCAL VOC 2012验证集上进行的模型消融研究。
For linear self-correction, α controls the weighting in the KL-divergence bias with large α favoring the ancillary model and small α favoring the primary model. We explored different starting and ending values for α with an exponential decay in-between. We ﬁnd that a starting value of α = 30 and the ﬁnal value of α = 0.5 performs well for both datasets. This parameter setting is robust as moderate changes of these values have little effect. 
对于线性自校正，α控制KL散度偏差的权重，其中较大的α有利于辅助模型，较小的α有利于初级模型。
3Note that, we do not initialize the parameters from a MS-COCO pretrained model. 
3请注意，我们不从MS-COCO预训练模型初始化参数。
Table 2 compares the performance of our models against different baselines and published results. In this experiment, we use 1464 images as F and 9118 images originally from the auxiliary dataset as W . Both self-correction models achieve similar results and outperform other models. Surprisingly, our semi-supervised models outperform the fully supervised model. We hypothesize two possible explanations for this observation. Firstly, this may be due to label noise in the 9k auxiliary set [18] that negatively affects performance of Vanilla DeepLapv3+. As ev
表2比较了不同基准和已发布结果的模型性能。
12720 
12720\"]\n]\n]\n]\n,\"zh\",1,\"auto
Data Split 
数据分割
F W 
体重
Method 
方法
Val Test 
Val测试
1464 9118 No Self-Corr. 80.34 81.61 1464 9118 Lin. Self-Corr. 81.35 81.97 1464 9118 Conv. Self-Corr. 82.33 82.72 1464 9118 EM-ﬁxed Ours [41] 79.25 -10582 - Vanilla DeepLabv3+ [9] 81.21 -
1464 9118没有自我更正
1464 9118 BoxSup-MCG [12] 1464 9118 EM-ﬁxed [41] 1464 9118 M
1464 9118 BoxSup-MCG [12] 1464 9118 EM固定[41] 1464 9118 M
63.5 65.1 
63.5 65.1\"]\n]\n]\n]\n,\"zh\",1,\"auto
--
--\"]\n]\n]\n]\n,\"zh\",1,\"auto
idence, Fig. 5 compares the output of the ancillary model with ground-truth annotations and highlights some of improperly labeled instances. Secondly, the performance gain may also be due to explicit modeling of label uncertainty and self-correction. To test this hypothesis, we train vanilla DeepLabv3+ on only 1.4K instances in the original PASCAL VOC 2012 training set4 and obtain 68.8% mAP on the validation set. However, if we train the convolutional self-correction model on the same training set and allow the model to reﬁne the ground truth labels using self-correction5 , we get mAP as high as 76.88% (the convolutional self correction on top of bounding boxes yields 75.97% mAP). This indicates that modeling noise with robust loss functions and allowing for self-correction may signiﬁcantly improve the performance of segmentation models. This is consonant with self-correction approaches that have been shown to be effective for edge detection [66, 2], and is in contrast to common segmentation objectives which train models using cross-entropy with one-hot annotation masks. Very similar to our approach and reasoning, [67] uses logits to train a lightweight pose estimation model using knowledge distillation technique. Unfortunately, the state-of-the-art models are still using the older versions of DeepLab. It was infeasible for us to either re-implement most of these approaches using DeepLabv3+ or re-implement our work using old versions. The only exception is EM-ﬁxed baseline [41]. Our re-implementation using DeepLabv3+ achieves 79.25% on the validation set while the original paper has reported 64.6% using DeepLabv1. In the lower half of Table 2, we record previously published results (using older versions of DeepLab). A careful examination of the results show that our work is superior to previous work as our semisupervised models outperform the fully supervised model while previous work normally do not. Finally, comparing Table 1 and 2, we see that with F = 200 and W = 10382, our linear self-correction model performs similarly to DeepLabv3+ trained with the whole dataset. Using the labeling cost reported in [5], this theoretically translates to a ∼7x reduction in annotation cost. 
根据证据，图5将辅助模型的输出与真实的注释进行了比较，并突出显示了一些标签不正确的实例。其次，性能提升还可能归因于标签不确定性和自我校正的显式建模。为了验证这一假设，我们在原始PASCAL VOC 2012训练集中4的仅1.4K实例上训练了香草DeepLabv3 +，并在验证集中获得了68.8％的mAP。但是，如果我们在同一训练集上训练卷积自校正模型，并允许该模型使用自校正对地面真相标签进行细化，则mAP会高达76.88％（边界框顶部的卷积自校正会产生75.97％（mAP）。这表明使用鲁棒的损失函数对噪声进行建模并允许自我校正可以显着提高分割模型的性能。这与已经显示出对边缘检测有效的自校正方法是一致的[66，2]，这与常见的分割目标相反，后者使用带有单热点注释蒙版的交叉熵训练模型。与我们的方法和推理非常相似，[67]使用logits来利用知识提炼技术来训练轻量级的姿势估计模型。不幸的是，最新的模型仍在使用DeepLab的旧版本。对于我们而言，要么使用DeepLabv3 +重新实现大多数这些方法，要么使用旧版本重新实现我们的工作是不可行的。唯一的例外是EM固定基线[41]。我们使用DeepLabv3 +的重新实现在验证集上达到了79.25％，而原始论文报告的使用DeepLabv1的达到了64.6％。在表2的下半部分，我们记录了以前发布的结果（使用DeepLab的较旧版本）。仔细检查结果表明，我们的工作优于以前的工作，因为我们的半监督模型优于完全监督的模型，而以前的工作通常没有。最后，通过比较表1和表2，我们看到在F \\u003d 200和W \\u003d 10382的情况下，我们的线性自校正模型的性能类似于对整个数据集训练的DeepLabv3 +。使用[5]中报道的标签成本，从理论上讲，这将使注释成本降低约7倍。\"]\n]\n]\n]\n,\"zh\",1,\"auto
4.2. Cityscapes Dataset 
4.2。
In this section we evaluate performance on the Cityscapes dataset [11] which contains images collected from cars driving in cities during different seasons. This dataset has good quality annotations, however some instances are over/under segmented. It consists of 2975 training, 500 validation, and 1525 test images covering 19 foreground object classes (stuff and object) for the segmentation 
在本节中，我们评估Cityscapes数据集[11]的性能，该数据集包含从不同季节在城市驾驶的汽车收集的图像。
4 The auxiliary set is excluded to avoid potentially noisy labels. 5 For this experiment 1.1K images are used as F and 364 images as W . For W , we let self-correction model to reﬁne the original groundtruth labels. 
4排除了辅助设备，以避免可能产生噪音的标签。
Input Image 
输入图像
ground-truth 
真相
Ancillary Model No Self-correction Lin. Self-correction Conv. Self-correction 
辅助型号无自我校正功能。
Figure 4: Qualitative results on the PASCAL VOC 2012 validation set. The last four columns represent the models in column 1464 of Table 1. The Conv. Self-correction model typically segments objects better than other models. 
图4：PASCAL VOC 2012验证集中的定性结果。
Input Image 
输入图像
Ground-truth 
真相
Ancillary Heatmap 
辅助热图
Input Image 
输入图像
Ground-truth 
真相
Ancillary Heatmap 
辅助热图
Figure 5: Qualitative results on the PASCAL VOC 2012 auxiliary (the weak set). The heatmap of a single class for the ancillary model is shown for several examples. The ancillary model can successfully correct the labels for missing or oversegmented objects in these images (marked by ellipses). 
图5：PASCAL VOC 2012辅助程序（弱集）的定性结果。
Table 3 reports the performance of our model for an increasing number of images as F , and Table 4 compares our models with several baselines similar to the previous dataset. The same conclusion and insights observed on the PASCAL dataset hold for the Cityscapes dataset indicating the efﬁcacy of our self-corrective framework. 
表3报告了随着F越来越多的图像，我们的模型的性能，表4将我们的模型与与先前数据集相似的几个基线进行了比较。
5. Conclusion 
5.结论
In this paper, we have proposed a semi-supervised framework for training deep CNN segmentation models using a small set of fully labeled and a set of weakly labeled 
在本文中，我们提出了一个半监督框架，用于使用一小组完全标记的样本和一组弱标记的样本来训练深度CNN分割模型
images (boxes annotations only). We introduced two mechanisms that enable the underlying primary model to correct the weak labels provided by an ancillary model. The proposed self-correction mechanisms combine the predictions made by the primary and ancillary model either using a linear function or trainable CNN. The experiments show that our proposed framework outperforms previous semisupervised models on both the PASCAL VOC 2012 and Cityscapes datasets. Our framework can also be applied to the instance segmentation task [21, 74, 72], but we leave further study of this to future work. 
图片（仅用于框注释）。
12722 
12722\"]\n]\n]\n]\n,\"zh\",1,\"auto
References 
参考文献
[1] Mart´ın Abadi, Ashish Agarwal, and et al. Tensorﬂow: Largescale machine learning on heterogeneous distributed systems. 2016. 6 
[1] Mart´ın Abadi，Ashish Agarwal等。
[2] David Acuna, Amlan Kar, and Sanja Fidler. Devil is in the edges: Learning semantic boundaries from noisy annotations. In CVPR, 2019. 7 
[2] David Acuna，Amlan Kar和Sanja Fidler。
[3] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic afﬁnity with image-level supervision for weakly supervised semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2 
[3] Jiwoon Ahn和Suha Kwak。
[4] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. 2015. 1, 2 
[4] Vijay Badrinarayanan，Alex Kendall和Roberto Cipolla。
[5] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei. What’s the point: Semantic segmentation with point supervision. In European Conference on Computer Vision (ECCV), 2016. 1, 2, 7 
[5]艾米·比尔曼，奥尔加·鲁萨科夫斯基，维托里奥·法拉利和李飞飞。
[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. In International Conference on Learning Representations (ICLR), 2015. 2, 7 
[6] Chen-Lieh Chen，George Papandreou，Iasonas Kokkinos，Kevin Murphy和Alan L. Yuille。
[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017. 2
[7] Chen-Lieh Chen，George Papandreou，Iasonas Kokkinos，Kevin Murphy和Alan L. Yuille。
[8] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. 2017. 2 
[8] Chen-Lieh Chen，George Papandreou，Florian Schroff和Hartwig Adam。
[9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In European Conference on Computer Vision (ECCV), 2018. 1, 2, 5, 6, 7 
[9] Chen Liang-Chieh Chen，Yukun Zhu，George Papandreou，Florian Schroff和Hartwig Adam。
[10] Franc¸ ois Chollet. Xception: Deep learning with depthwise separable convolutions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2 
[10]Franc¸ois Chollet。
[11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 7 
[11] Marius Cordts，Mohamed Omran，Sebastian Ramos，Timo Rehfeld，Markus Enzweiler，Rodrigo Benenson，Uwe Franke，Stefan Roth和Bernt Schiele。
[12] Jifeng Dai, Kaiming He, and Jian Sun. Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation. In IEEE International Conference on Computer Vision (ICCV), 2015. 1, 2, 3, 4, 7 
[12]戴继峰，何开明和孙健。
[13] Mostafa Dehghani, Arash Mehrjou, Stephan Gouws, Jaap Kamps, and Bernhard Sch ¨olkopf. Fidelity-weighted learning. In International Conference on Learning Representations (ICLR), 2018. 2 
[13] Mostafa Dehghani，Arash Mehrjou，Stephan Gouws，Jaap Kamps和BernhardSch¨olkopf。
[14] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, 
[14] Jeffrey Donhue，Lisa Anne Hendricks，Sergio Guadarrama，Marcus Rohrbuch，Subhashini Venugopalan，Katte Sanco，
and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2625–2634, 2015. 1 [15] Thibaut Durand, Taylor Mordan, Nicolas Thome, and Matthieu Cord. WILDCAT: weakly supervised learning of deep convnets for image classiﬁcation, pointwise localization and segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2 [16] Mark Everingham, S. M. Ali Eslami, Luc J. Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. The pascal visual object classes challenge: A retrospective. International Journal of Computer Vision (IJCV), 2015. 6 [17] Weifeng Ge, Sibei Yang, and Yizhou Yu. Multi-evidence ﬁltering and fusion for multi-label classiﬁcation, object detection and semantic segmentation based on weakly supervised learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2 [18] Bharath Hariharan, Pablo Arbelaez, Lubomir D. Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In IEEE International Conference on Computer Vision (ICCV), 2011. 6 [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. 1 [20] Seunghoon Hong, Donghun Yeo, Suha Kwak, Honglak Lee, and Bohyung Han. Weakly supervised semantic segmentation using web-crawled videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2 [21] Ronghang Hu, Piotr Dollr, Kaiming He, Trevor Darrell, and Ross Girshick. Learning to segment every thing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 8 [22] Zilong Huang, Xinggang Wang, Jiasi Wang, Wenyu Liu, and Jingdong Wang. Weakly-supervised semantic segmentation network with deep seeded region growing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2 [23] W.-C. Hung, Y.-H. Tsai, Y.-T. Liou, Y.-Y. Lin, and M.H. Yang. Adversarial learning for semi-supervised semantic segmentation. In British Machine Vision Conference (BMVC), 2018. 2 [24] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Regularizing very deep neural networks on corrupted labels. In International Conference on Machine Learning (ICML), 2018. 2 [25] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classiﬁcation with convolutional neural networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725–1732, 2014. 1 [26] Anna Khoreva, Rodrigo Benenson, Jan Hendrik Hosang, Matthias Hein, and Bernt Schiele. Simple does it: Weakly supervised instance and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 1, 2, 3, 4, 7 
和Trevor Darrell。用于视觉识别和描述的长期递归卷积网络。在IEEE关于计算机视觉和模式识别的会议论文集，第2625–2634页，2015年。1 [15] Thibaut Durand，Taylor Mordan，Nicolas Thome和Matthieu Cord。 WILDCAT：深度监督的弱监督学习，用于图像分类，逐点定位和分割。在IEEE计算机视觉和模式识别会议（CVPR）中，2017。2 [16] Mark Everingham，S.M。Ali Eslami，Luc J. Van Gool，Christopher K. I. Williams，John M. Winn和Andrew Zisserman。帕斯卡视觉对象类的挑战：回顾。国际计算机视觉杂志（IJCV），2015年。6 [17]葛卫峰，杨思蓓，余益州。基于弱监督学习的用于多标签分类，对象检测和语义分割的多证据过滤和融合。在IEEE计算机视觉和模式识别会议（CVPR）中，2018年。2 [18] Bharath Hariharan，Pablo Arbelaez，Lubomir D.Bourdev，Subhransu Maji和Jitendra Malik。逆检测器的语义轮廓。在IEEE国际计算机视觉会议（ICCV）中，2011。6 [19]何凯明，张向宇，任少清和孙健。深度残差学习，用于图像识别。在计算机视觉和模式识别（CVPR）中，第770-778页，2016年。1 [20]洪升勋，杨东勋，郭苏Su，李洪立和韩波兴。使用网络爬行视频弱监督语义分割。在IEEE计算机视觉和模式识别会议（CVPR）中，2017。2 [21]胡荣航，Piotr Dollr，何凯明，Trevor Darrell和Ross Girshick。学习将每一件事都细分。在IEEE计算机视觉和模式识别会议（CVPR）中，2018。8 [22]黄子龙，王星刚，王佳思，刘文宇和王京东。具有弱种子区域增长的弱监督语义分割网络。在IEEE计算机视觉和模式识别会议（CVPR）中，2018年。2 [23] W.-C.洪永辉蔡，Y.-T。刘ou林和M.H.杨半监督语义分割的对抗学习。在英国机器视觉会议（BMVC）中，2018年。2 [24]陆江，周正远，梁朝伟，李丽嘉和李飞飞。 Mentornet：在损坏的标签上规范非常深的神经网络。在国际机器学习会议（ICML）中，2018年。2 [25] Andrej Karpathy，George Toderici，Sanketh Shetty，Thomas Leung，Rahul Sukthankar和Li Fei-Fei。带卷积神经网络的大规模视频分类。在IEEE计算机视觉和模式识别会议论文集，第1725–1732页，2014年。1 [26] Anna Khoreva，Rodrigo Benenson，Jan Hendrik Hosang，Matthias Hein和Bernt Schiele。做起来很简单：弱监督实例和语义分割。在IEEE计算机视觉和模式识别会议（CVPR）中，2017.1、2、3、4、7\"]\n]\n]\n]\n,\"zh\",1,\"auto
12723 
12723\"]\n]\n]\n]\n,\"zh\",1,\"auto
[27] Philipp Kr ¨ahenb ¨uhl and Vladlen Koltun. Efﬁcient inference in fully connected CRFs with Gaussian edge potentials. In Advances in Neural Information Processing Systems (NIPS), 2011. 2 [28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Neural Information Processing Systems (NIPS), pages 1097–1105, 2012. 1 [29] Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. Beyond bags of features: spatial pyramid matching for recognizing natural scene categories. In Conference on Computer Vision and Pattern Recognition (CPRV), pages 2169–2178. IEEE Computer Society, 2006. 2 [30] Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, and Sungroh Yoon. Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference. In Computer Vision and Pattern Recognition (CVPR), 2019. 7 [31] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised convolutional networks for semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2 [32] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian D. Reid. Reﬁnenet: Multi-path reﬁnement networks for highresolution semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2 [33] Guosheng Lin, Chunhua Shen, Anton Van Den Hengel, and Ian Reid. Efﬁcient piecewise training of deep structured models for semantic segmentation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2 [34] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21–37. Springer, 2016. 1 [35] Wei Liu, Andrew Rabinovich, and Alexander Berg. Parsenet: Looking wider to see better. arXiv preprint arXiv:1506.04579, 2015. 2 [36] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, and Xiaoou Tang. Semantic image segmentation via deep parsing network. In IEEE International Conference on Computer Vision (ICCV), 2015. 2 [37] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. 2 [38] Ishan Misra, C. Lawrence Zitnick, Margaret Mitchell, and Ross Girshick. Seeing through the human reporting bias: Visual classiﬁers from noisy human-centric labels. In CVPR, 2016. 2 [39] Volodymyr Mnih and Geoffrey E. Hinton. Learning to label aerial images from noisy data. In International Conference on Machine Learning (ICML), pages 567–574, 2012. 2 [40] Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep K. Ravikumar, and Ambuj Tewari. Learning with noisy labels. In Advances in neural information processing systems, pages 1196–1204, 2013. 2 [41] George Papandreou, Liang-Chieh Chen, Kevin P. Murphy, and Alan L. Yuille. Weakly-and semi-supervised learning of 
[27]菲利普·科尔（Philipp Kr·ahenb·uhl）和弗拉德·科尔顿（Vladlen Koltun）。具有高斯边缘电势的全连接CRF的有效推断。神经信息处理系统（NIPS）的进展，2011年。2 [28] Alex Krizhevsky，Ilya Sutskever和Geoffrey E Hinton。深度卷积神经网络对图像网络进行分类。在神经信息处理系统（NIPS）中，第1097-1105页，2012年。1 [29] Svetlana Lazebnik，Cordelia Schmid和Jean Ponce。功能之外：空间金字塔匹配可识别自然场景类别。在计算机视觉和模式识别会议（CPRV）上，第2169–2178页。 IEEE计算机协会，2006年。2 [30] Lee Jungbeom，Eunji Kim，Sungmin Lee，Jangho Lee和Sungroh Yoon。 Ficklenet：使用随机推理的弱半监督语义图像分割。计算机视觉与模式识别（CVPR），2019年。7 [31]林迪，戴继峰，贾佳亚，何凯明和孙健。 Scribblesup：用于语义分割的乱涂监督的卷积网络。在IEEE计算机视觉和模式识别会议（CVPR）中，2016。2 [32]林国胜，安东·米兰，沉春华和Ian D. Reid。 Refinenet：用于高分辨率语义分割的多路径优化网络。在IEEE计算机视觉和模式识别会议（CVPR）中，2017。2 [33]林国胜，沉春华，安东·范登·亨格尔和伊恩·里德。对语义分割的深度结构化模型进行有效的分段训练。在2016年计算机视觉和模式识别会议（CVPR）中。2 [34]刘炜，德拉戈米尔·安格洛夫，杜米特鲁·埃尔罕，克里斯蒂安·塞格迪，斯科特·里德，傅成扬和亚历山大·C·伯格。 Ssd：单发多盒检测器。在欧洲计算机视觉会议上，第21–37页。 Springer，2016年。1 [35]刘炜，安德鲁·拉比诺维奇和亚历山大·伯格。 Parsenet：范围更广，视野更好。 arXiv预印本arXiv：1506.04579，2015。2 [36]刘子玮，李晓晓，罗平，陈改来，唐晓鸥。通过深度解析网络进行语义图像分割。在IEEE计算机视觉国际会议（ICCV）中，2015。2 [37] Jonathan Long，Evan Shelhamer和Trevor Darrell。用于语义分割的完全卷积网络。在IEEE计算机视觉和模式识别会议（CVPR）中，2015。2 [38] Ishan Misra，C。Lawrence Zitnick，Margaret Mitchell和Ross Girshick。观察人类报告的偏见：以人为中心的嘈杂标签的视觉分类。在CVPR中，2016。2 [39] Volodymyr Mnih和Geoffrey E. Hinton。学习从嘈杂的数据中标记航空影像。在国际机器学习会议（ICML）上，第567-574页，2012年。2 [40] Nagarajan Natarajan，Inderjit S. Dhillon，Pradeep K. Ravikumar和Ambuj Tewari。带有嘈杂标签的学习。在神经信息处理系统的进展中，第1196–1204页，2013年。2 [41] George Papandreou，Chen-Chieh Chen，Kevin P. Murphy和Alan L. Yuille。弱和半监督学习\"]\n]\n]\n]\n,\"zh\",1,\"auto
a deep convolutional network for semantic image segmentation. In IEEE International Conference on Computer Vision (ICCV), 2015. 1, 2, 3, 4, 5, 6, 7 
一个用于语义图像分割的深度卷积网络。
[42] Giorgio Patrini, Alessandro Rozza, Aditya Menon, Richard Nock, and Lizhen Qu. Making neural networks robust to label noise: A loss correction approach. In Computer Vision and Pattern Recognition, 2017. 2 
[42] Giorgio Patrini，Alessandro Rozza，Aditya Menon，Richard Nock和Lizhen Qu。
[43] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Uniﬁed, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779–788, 2016. 1 
[43]约瑟夫·雷德蒙（Joseph Redmon），桑托什·迪夫瓦拉（Santosh Divvala），罗斯·吉尔希克（Ross Girshick）和阿里·法哈迪（Ali Farhadi）。
[44] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In International Conference on Machine Learning (ICML), 2018. 2 
[44]任梦野，曾文远，杨斌和拉奎尔·乌尔塔孙。
[45] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91–99, 2015. 1 
[45]任少卿，何开明，罗斯·吉尔希克和孙健。
[46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2015. 2 
[46] Olaf Ronneberger，Philipp Fischer和Thomas Brox。
[47] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. Grabcut: Interactive foreground extraction using iterated graph cuts. In ACM transactions on graphics (TOG). ACM, 2004. 2, 3 
[47]卡斯滕·罗瑟（Carsten Rother），弗拉基米尔·科莫格洛夫（Vladimir Kolmogorov）和安德鲁·布雷克（Andrew Blake）。
[48] Alexander G Schwing and Raquel Urtasun. Fully connected deep structured networks. arXiv preprint arXiv:1503.02351, 2015. 2 
[48]亚历山大·G·史威格（Alexander G Schwing）和拉克尔·乌尔塔松（Raquel Urtasun）。
[49] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. In Advances in neural information processing systems, pages 568– 576, 2014. 1 
[49] Karen Simonyan和Andrew Zisserman。
[50] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. Box-driven class-wise region masking and ﬁlling rate guided loss for weakly supervised semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2019. 7 
[50]宋春凤，黄艳，欧阳万里和王亮。
[51] Nasim Souly, Concetto Spampinato, and Mubarak Shah. Semi supervised semantic segmentation using generative adversarial network. In IEEE International Conference on Computer Vision (ICCV), 2017. 2 
[51] Nasim Souly，Concetto Spampinato和Mubarak Shah。
[52] Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2014. 2 
[52]塞恩巴亚尔·苏克巴托，琼·布鲁纳，马诺哈·帕鲁里，卢博米尔·布尔德夫和罗布·弗格斯。
[53] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep convolutional network cascade for facial point detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3476–3483, 2013. 1 
[53]孙艺，王小刚和唐小鸥。
[54] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework for learning with noisy labels. In Computer Vision and Pattern Recognition (CVPR), 2018. 2, 4 
[54]田中大树（Daiki Tanaka），大树大树（Ikiami），山崎俊彦（Toshihiko Yamasaki）和相泽清治（Kiyoharu Aizawa）。
[55] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 
[55] Du Tran，Lubomir Bourdev，Rob Fergus，Lorenzo Torresani和Manohar Paluri。
12724 
12724\"]\n]\n]\n]\n,\"zh\",1,\"auto
[70] Zhilu Zhang and Mert R Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In Neural Information Processing Systems (NIPS), 2018. 2 [71] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 1, 2 [72] Xiangyun Zhao, Shuang Liang, and Yichen Wei. Pseudo mask augmented object detection. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 8 [73] Shuai Zheng, Sadeep Jayasumana, Bernardino RomeraParedes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip HS Torr. Conditional random ﬁelds as recurrent neural networks. In International Conference on Computer Vision (ICCV), pages 1529–1537, 2015. 2 [74] Yanzhao Zhou, Yi Zhu, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Weakly supervised instance segmentation using class peak response. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 8 [75] Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and Stan Z Li. Face alignment across large poses: A 3d solution. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 146–155, 2016. 1 [76] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable architectures for scalable image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 1 
[70] Zhang Zhilu和Mert R Sabuncu。用于训练带有噪声标签的深度神经网络的广义交叉熵损失。在神经信息处理系统（NIPS）中，2018。2 [71]赵恒双，石建平，齐晓娟，王晓刚，贾佳雅。金字塔场景解析网络。在IEEE计算机视觉和模式识别会议（CVPR）中，2017年。1，2 [72]赵向云，梁双和魏一辰。伪蒙版增强对象检测。在IEEE计算机视觉和模式识别会议（CVPR）中，2018年。8 [73]郑帅，萨迪普·贾亚苏马纳纳，贝纳迪诺·罗梅拉·帕雷德斯，Vibhav Vineet，苏志忠，杜大龙，张长和Philip HS Torr。有条件的随机场作为递归神经网络。在国际计算机视觉会议（ICCV）上，第1529-1537页，2015年。2 [74]周艳照，朱以，叶启祥，邱强和焦建斌。使用类峰值响应的弱监督实例分割。在IEEE计算机视觉和模式识别会议（CVPR）中，2018年。8 [75]朱向宇，雷震，刘晓明，石海林和Stan Z Li。大姿势时的面部对齐：3D解决方案。在IEEE计算机视觉和模式识别会议论文集中，第146-155页，2016年。1 [76] Barret Zoph，Vijay Vasudevan，Jonathon Shlens和Quoc V. Le。学习可转移的体系结构以实现可伸缩的图像识别。在2018年6月的IEEE计算机视觉和模式识别会议（CVPR）中。1\"]\n]\n]\n]\n,\"zh\",1,\"auto
3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 4489–4497, 2015. 1 [56] Arash Vahdat. Toward robustness against label noise in training deep discriminative neural networks. In Neural Information Processing Systems (NIPS), 2017. 1, 2, 4, 6 [57] Arash Vahdat and Greg Mori. Handling uncertain tags in visual recognition. In International Conference on Computer Vision (ICCV), 2013. 2 [58] Arash Vahdat, Guang-Tong Zhou, and Greg Mori. Discovering video clusters from visual features and noisy tags. In European Conference on Computer Vision (ECCV), 2014. 2 [59] Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie. Learning from noisy large-scale datasets with minimal supervision. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6575–6583. IEEE, 2017. 2 [60] Xiang Wang, Shaodi You, Xi Li, and Huimin Ma. Weaklysupervised semantic segmentation by iteratively mining common object features. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2 [61] Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, and Shuicheng Yan. Object region mining with adversarial erasing: A simple classiﬁcation to semantic segmentation approach. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2 [62] Yunchao Wei, Huaxin Xiao, Honghui Shi, Zequn Jie, Jiashi Feng, and Thomas S. Huang. Revisiting dilated convolution: A simple approach for weakly- and semi- supervised semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2 [63] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classiﬁcation. In Computer Vision and Pattern Recognition (CVPR), 2015. 2 [64] Jia Xu, Alexander G. Schwing, and Raquel Urtasun. Learning to segment under various forms of weak supervision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. 2 [65] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In International Conference on Learning Representations (ICLR), 2015. 2 [66] Zhiding Yu, Weiyang Liu, Yang Zou, Chen Feng, Srikumar Ramalingam, BVK Vijaya Kumar, and Jan Kautz. Simultaneous edge alignment and learning. In European Conference on Computer Vision (ECCV), 2018. 7 [67] Feng Zhang, Xiatian Zhu, and Mao Ye. Fast human pose estimation. In Computer Vision and Pattern Recognition (CVPR), 2019. 7 [68] Wei Zhang, Sheng Zeng, Dequan Wang, and Xiangyang Xue. Weakly supervised semantic segmentation for social images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. 2 [69] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial landmark detection by deep multi-task learning. In European Conference on Computer Vision, pages 94–108. Springer, 2014. 1 
3d卷积网络。在IEEE计算机视觉国际会议论文集，第4489-4497页，2015年。1 [56] Arash Vahdat。在训练深层判别神经网络中提高抗标签噪声的鲁棒性。在神经信息处理系统（NIPS）中，2017。1、2、4、6 [57] Arash Vahdat和Greg Mori。处理视觉识别中的不确定标签。在国际计算机视觉会议（ICCV）中，2013。2 [58] Arash Vahdat，周光彤和Greg Mori。从视觉特征和嘈杂的标签中发现视频集群。在2014年欧洲计算机视觉会议（ECCV）中。2 [59] Andreas Veit，Neil Alldrin，Gal Chechik，Ivan Krasin，Abhinav Gupta和Serge Belongie。从嘈杂的大规模数据集中学习，而无需进行监督。在2017年IEEE计算机视觉和模式识别会议（CVPR）中，第6575–6583页。 IEEE，2017年。2 [60]王翔，尤少迪，李希和马慧敏。通过迭代地挖掘常见对象特征来弱监督语义分割。在IEEE计算机视觉和模式识别会议（CVPR）中，2018。2 [61]魏云超，冯家石，梁晓丹，成明明，赵瑶和闫水成。具有对抗性擦除的对象区域挖掘：一种简单的语义分割方法分类。在IEEE计算机视觉和模式识别会议（CVPR）中，2017年。2 [62]魏云超，肖华新，石宏辉，杰泽群，冯佳石和黄Thomas雄。重温膨胀卷积：一种用于弱监督和半监督语义分割的简单方法。在IEEE计算机视觉和模式识别会议（CVPR）中，2018年。2 [63]童潇，田霞，杨奕，黄昌和王小刚。从大量带噪标签数据中学习以进行图像分类。计算机视觉与模式识别（CVPR），2015年。2 [64]贾旭，亚历山大·G·施温（Alexander G. Schwing）和拉克尔·乌尔塔松（Raquel Urtasun）。学会在各种形式的弱监督下进行细分。在IEEE计算机视觉和模式识别会议（CVPR）中，2015。2 [65] Fisher Yu和Vladlen Koltun。通过扩展卷积进行多尺度上下文聚合。在国际学习代表大会（ICLR）中，2015。2 [66]于志定，刘未央，邹扬，陈峰，Srikumar Ramalingam，BVK Vijaya Kumar和Jan Kautz。同时进行边缘对齐和学习。在欧洲计算机视觉会议（ECCV）中，2018年。7 [67]张峰，朱夏田和毛烨。快速的人体姿势估计。计算机视觉与模式识别（CVPR），2019年。7 [68]张巍，曾胜，王德全和薛向阳。弱监督的社交图像语义分割。在IEEE计算机视觉和模式识别会议（CVPR）中，2015年。2 [69]张占鹏，罗平，陈改来和唐小鸥。通过深度多任务学习进行人脸标志检测。在欧洲计算机视觉会议上，第94-108页。施普林格，2014年。1\"]\n]\n]\n]\n,\"zh\",1,\"auto
12725 
12725\"]\n]\n]\n]\n,\"zh\",1,\"auto
